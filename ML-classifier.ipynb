{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d46f644",
   "metadata": {},
   "source": [
    "### GPU Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cbd45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check if GPU is available\n",
    "print(\"Num GPUs Available:\", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "# Set memory growth to avoid TensorFlow consuming all GPU memory\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"GPU memory growth set\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd6aea7",
   "metadata": {},
   "source": [
    "### Import and package install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90979ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # for dealing with data\n",
    "from scipy.signal import butter, sosfiltfilt, sosfreqz  # for filtering\n",
    "import matplotlib.pyplot as plt                         # for plotting\n",
    "import seaborn as sns\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join, isdir\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Permute, Dropout\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from tensorflow.keras.layers import SeparableConv2D, DepthwiseConv2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import SpatialDropout2D\n",
    "from tensorflow.keras.layers import Input, Flatten\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import utils as np_utils\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import pandas as pd\n",
    "import cupy as cp\n",
    "import cudf\n",
    "import seaborn as sns\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "# METRIC\n",
    "from sklearn.metrics import (roc_curve, precision_score, recall_score,\n",
    "                             f1_score, balanced_accuracy_score, auc)\n",
    "from cuml.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "# OVERSAMPLING\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# MODEL\n",
    "from cuml.linear_model import LogisticRegression as cuLR\n",
    "from cuml.svm import SVC as cuSVC\n",
    "from cuml.ensemble import RandomForestClassifier as cuRF\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "!pip install PyWavelets\n",
    "!pip install pyriemann"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9a28dc",
   "metadata": {},
   "source": [
    "### Filter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e0059a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 200.0     # 200 Hz sampling rate\n",
    "lowcut = 1.0\n",
    "highcut = 50.0\n",
    "\n",
    "#! For butterworth band pass filter\n",
    "def butter_bandpass_filter(raw_data, fs, lowcut = 1.0, highcut = 40.0, order = 5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    sos = butter(order, [low, high], analog = False, btype = 'band', output = 'sos')\n",
    "    filted_data = sosfiltfilt(sos, raw_data)\n",
    "    return filted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d69c2a0",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8419dab4",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.6' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "epoch_s = -100      # epoch starting time relative to stmulus in miliseconds\n",
    "epoch_e = 600    # epoch ending time relative to stmulus in miliseconds\n",
    "bl_s = -100         # baseline starting time relative to stmulus in miliseconds\n",
    "bl_e = 0       # baseline ending time relative to stmulus in miliseconds\n",
    "epoch_len = int((abs(epoch_s) + abs(epoch_e)) * (fs / 1000))\n",
    "print(epoch_len)\n",
    "\n",
    "train_subj_num = 16\n",
    "test_subj_num = 10\n",
    "stimulus_per_subj = 340\n",
    "trial_per_subj = 5\n",
    "\n",
    "channels = [\n",
    "    'Fp1', 'Fp2', 'F7', 'F3', 'Fz', 'F4', 'F8',\n",
    "    'FT7', 'FC3', 'FCz', 'FC4', 'FT8',\n",
    "    'T7', 'C3', 'Cz', 'C4', 'T8',\n",
    "    'TP7', 'CP3', 'CPz', 'CP4', 'TP8',\n",
    "    'P7', 'P3', 'Pz', 'P4', 'P8',\n",
    "    'O1', 'POz', 'O2'\n",
    "]\n",
    "print(len(channels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e484ddb1",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0926f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_list = np.load('./data/train_data.npy')\n",
    "test_data_list = np.load('./data/test_data.npy')\n",
    "print('Epoched training data shape: ' + str(train_data_list.shape)) #! (16, 340, 56, 140)\n",
    "print('Epoched training data: ' , train_data_list)\n",
    "print('Epoched testing data shape: ' + str(test_data_list.shape)) #! (10, 340, 56, 140)\n",
    "print('Epoched testing data: ' , test_data_list)\n",
    "\n",
    "Y_train_valid = pd.read_csv('data/TrainLabels.csv')['Prediction'].values\n",
    "Y_true_labels = pd.read_csv('./data/true_labels.csv', header=None).values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c48ef42",
   "metadata": {},
   "source": [
    "### Reshape data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7ad954",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj, trial, numChannel, sample = train_data_list.shape #! (16, 340, 30, 140)\n",
    "X_train_valid = np.reshape(train_data_list, (-1, 1, numChannel, sample)) #! (5440, 1, 30, 140)\n",
    "X_test = np.reshape(test_data_list, (-1, 1, numChannel, sample)) #! (3400, 1, 30, 140)\n",
    "\n",
    "Y_true_Labels_for_test = np.reshape(Y_true_labels, 3400)\n",
    "\n",
    "print('subject: ', subj)\n",
    "print('trial: ', trial)\n",
    "print('numChannel: ', numChannel)\n",
    "print('sample: ', sample)\n",
    "print('X_train_valid: ', X_train_valid.shape)\n",
    "print('X_test: ', X_test.shape)\n",
    "print('Y_train_valid: ', Y_train_valid.shape)\n",
    "print('Y_true_Labels_for_test: ', Y_true_Labels_for_test.shape) #! (3400,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa33ee4",
   "metadata": {},
   "source": [
    "### Mix precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2140e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b734f544",
   "metadata": {},
   "source": [
    "### Time tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ca7254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def timed_block(name=\"Block\"):\n",
    "    start = time()\n",
    "    yield\n",
    "    end = time()\n",
    "    print(f\"üïí [{name}] Ëä±Ë≤ªÊôÇÈñì: {end - start:.2f} Áßí\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d8d4cd",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0efc8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import stft\n",
    "import pywt\n",
    "def extract_stft_wavelet_features(data, sampling_rate=256, wavelet='db4', stft_nperseg=64):\n",
    "    \"\"\"\n",
    "    ÁµêÂêà STFT + Wavelet Transform ÁöÑ EEG ÁâπÂæµÊì∑Âèñ\n",
    "\n",
    "    Parameters:\n",
    "        data (np.ndarray): EEG Ë≥áÊñô shape (n_subjects, n_channels, n_samples)\n",
    "        sampling_rate (int): EEG ÁöÑÂèñÊ®£È†ªÁéá\n",
    "        wavelet (str): Â∞èÊ≥¢Á®ÆÈ°ûÔºåÈ†êË®≠ÁÇ∫ Daubechies 4 ('db4')\n",
    "        stft_nperseg (int): STFT ÊØèÂÄãÁ™óÊ†ºÁöÑÊ®£Êú¨Êï∏\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: ÊØè‰ΩçÂèóË©¶ËÄÖÁöÑÂ±ïÂπ≥ÁâπÂæµ shape (n_subjects, total_features)\n",
    "    \"\"\"\n",
    "    n_subjects, n_channels, n_samples = data.shape\n",
    "    all_features = []\n",
    "\n",
    "    for subject in range(n_subjects):\n",
    "        # Èï∑Â∫¶ = n_channels √ó (n_freq √ó 3 + wavelet_level √ó 3)\n",
    "        subject_features = []\n",
    "\n",
    "        for channel in range(n_channels):\n",
    "            signal = data[subject, channel, :]\n",
    "\n",
    "            # === STFT ÁâπÂæµ ===\n",
    "            # Zxx ÊòØ STFT Ë§áÊï∏ÂÄºÈ†ªË≠úÔºöshape ÊòØ (frequencies, time_segments)\n",
    "            f, t, Zxx = stft(signal, fs=sampling_rate, nperseg=stft_nperseg)\n",
    "            stft_magnitude = np.abs(Zxx) # ÊØèÂÄãÊôÇÈñìÁ™óËàáÈ†ªÁéáÈªûÁöÑ magnitudeÔºàËÉΩÈáèÂº∑Â∫¶Ôºâ\n",
    "            stft_mean = np.mean(stft_magnitude, axis=1)  # ÂêÑÈ†ªÁéáÁöÑÂπ≥Âùá magnitudeÔºàË∑®ÊôÇÈñìÂπ≥ÂùáÔºâ\n",
    "            stft_std = np.std(stft_magnitude, axis=1) # ÂêÑÈ†ªÁéáÁöÑÊ®ôÊ∫ñÂ∑ÆÔºàÁ©©ÂÆöÊÄß/ËÆäÂåñÊÄßÔºâ\n",
    "            stft_energy = np.sum(stft_magnitude ** 2, axis=1) # ÂêÑÈ†ªÁéáÁöÑËÉΩÈáè\n",
    "            subject_features.extend(stft_mean)\n",
    "            subject_features.extend(stft_std)\n",
    "            subject_features.extend(stft_energy)\n",
    "\n",
    "            # === Wavelet ÁâπÂæµ ===\n",
    "            coeffs = pywt.wavedec(signal, wavelet, level=3)  # Â§öÂ±§ÂàÜËß£Ôºålevel Ë®≠ 3 ÊÄïÊúÉÊúâ boundary effects\n",
    "            for c in coeffs:\n",
    "                subject_features.append(np.mean(c))\n",
    "                subject_features.append(np.std(c))\n",
    "                subject_features.append(np.sum(np.square(c)))  # energy\n",
    "\n",
    "        all_features.append(subject_features)\n",
    "    # Ëº∏Âá∫ shape ÊòØ (n_subjects, total_features)\n",
    "    return np.array(all_features)\n",
    "\n",
    "def extract_stft_features(data, sampling_rate=256, stft_nperseg=64):\n",
    "    \"\"\"\n",
    "    Âè™‰ΩøÁî® STFT ÁöÑ EEG ÁâπÂæµÊì∑Âèñ\n",
    "\n",
    "    Parameters:\n",
    "        data (np.ndarray): EEG Ë≥áÊñô shape (n_subjects, n_channels, n_samples)\n",
    "        sampling_rate (int): EEG ÁöÑÂèñÊ®£È†ªÁéá\n",
    "        stft_nperseg (int): STFT ÊØèÂÄãÁ™óÊ†ºÁöÑÊ®£Êú¨Êï∏\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: ÊØè‰ΩçÂèóË©¶ËÄÖÁöÑÂ±ïÂπ≥ÁâπÂæµ shape (n_subjects, total_features)\n",
    "    \"\"\"\n",
    "    n_subjects, n_channels, n_samples = data.shape\n",
    "    all_features = []\n",
    "\n",
    "    for subject in range(n_subjects):\n",
    "        subject_features = []\n",
    "\n",
    "        for channel in range(n_channels):\n",
    "            signal = data[subject, channel, :]\n",
    "\n",
    "            # === STFT ÁâπÂæµ ===\n",
    "            f, t, Zxx = stft(signal, fs=sampling_rate, nperseg=stft_nperseg)\n",
    "            stft_magnitude = np.abs(Zxx)\n",
    "            stft_log = np.log1p(stft_magnitude)\n",
    "            stft_mean = np.mean(stft_log, axis=1)\n",
    "            stft_std = np.std(stft_log, axis=1)\n",
    "            stft_energy = np.sum(stft_log ** 2, axis=1)\n",
    "\n",
    "            subject_features.extend(stft_mean)\n",
    "            subject_features.extend(stft_std)\n",
    "            subject_features.extend(stft_energy)\n",
    "\n",
    "        all_features.append(subject_features)\n",
    "\n",
    "    return np.array(all_features)\n",
    "\n",
    "from scipy.fft import fft\n",
    "def fft_eeg(data, sampling_rate=256):\n",
    "  \"\"\"\n",
    "  Parameters:\n",
    "      data (numpy.ndarray): 3D EEG data of shape (n_subjects, n_channel, n_samples)\n",
    "      sampling_rate (int): Sampling rate of the EEG data (default: 256 Hz)\n",
    "\n",
    "  Returns:\n",
    "      numpy.ndarray: Flattened FFT features of shape (n_subjects, n_channel * n_freq_bins)\n",
    "  \"\"\"\n",
    "  # Á¢∫‰øùËº∏ÂÖ•Êï∏ÊìöÊòØ‰∏âÁ∂≠\n",
    "  assert len(data.shape) == 3, \"Input data must be 3-dimensional (n_subjects, n_channel, n_samples)\"\n",
    "  n_subjects, n_channels, n_samples = data.shape\n",
    "\n",
    "  # È†ªÁéáÁØÑÂúç\n",
    "  freqs = np.fft.fftfreq(n_samples, d=1/sampling_rate)\n",
    "  positive_freqs = freqs[:n_samples // 2]  # Âè™‰øùÁïôÊ≠£È†ªÁéáÈÉ®ÂàÜ\n",
    "  n_freq_bins = len(positive_freqs)\n",
    "\n",
    "  # ÂàùÂßãÂåñÁµêÊûúÁü©Èô£ (n_subjects, n_channels * n_freq_bins)\n",
    "  fft_features = np.zeros((n_subjects, n_channels * n_freq_bins))\n",
    "\n",
    "  for subject in range(n_subjects):\n",
    "      feature_list = []\n",
    "      for channel in range(n_channels):\n",
    "          # Ë®àÁÆó FFTÔºåÂèñÁµïÂ∞çÂÄº‰∏¶‰øùÁïôÊ≠£È†ªÁéáÈÉ®ÂàÜ\n",
    "          fft_values = np.abs(fft(data[subject, channel, :]))[:n_samples // 2]\n",
    "          # Ê®ôÊ∫ñÂåñÁâπÂæµ\n",
    "          fft_values /= np.sum(fft_values)  # ËÉΩÈáèÊ≠∏‰∏ÄÂåñ\n",
    "          # Ê∑ªÂä†Âà∞ÁâπÂæµÂàóË°®\n",
    "          feature_list.extend(fft_values)\n",
    "\n",
    "      # Â∞áÊØèÂÄãÂèóË©¶ËÄÖÁöÑÁâπÂæµËΩâÁÇ∫‰∏ÄÁ∂≠ÂêëÈáè\n",
    "      fft_features[subject, :] = feature_list\n",
    "\n",
    "  return fft_features\n",
    "\n",
    "from pyriemann.estimation import XdawnCovariances\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "def preprocess_riemann_features(\n",
    "    train_data, test_data, Y_train, nfilter=5):\n",
    "    \"\"\"\n",
    "    Preprocess EEG data using Xdawn + Tangent Space\n",
    "\n",
    "    Parameters:\n",
    "        train_data (np.ndarray): Training data of shape (n_subjects, n_channels, n_samples)\n",
    "        test_data (np.ndarray): Testing data of shape (n_subjects, n_channels, n_samples)\n",
    "        nfilter (int): Number of Xdawn spatial filters\n",
    "    \"\"\"\n",
    "    # Apply Xdawn and Tangent Space\n",
    "    XC = XdawnCovariances(nfilter=nfilter)\n",
    "    TS = TangentSpace(metric='riemann')\n",
    "\n",
    "    X_train = XC.fit_transform(train_data, Y_train)\n",
    "    X_train = TS.fit_transform(X_train)\n",
    "\n",
    "    X_test = XC.transform(test_data)\n",
    "    X_test = TS.transform(X_test)\n",
    "\n",
    "    return X_train, X_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f897f3",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4ebff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Need to modify\n",
    "denoising_method = 'ASR'\n",
    "\n",
    "# Smapling Rate (IC-U-Net: 256, Others: 200)\n",
    "SR = 200\n",
    "\n",
    "def initialize_metrics():\n",
    "  return {\n",
    "    \"max\": {k: 0 for k in [\"accuracy\", \"accuracy_balanced\", \"precision\", \"recall\", \"f1\", \"auc\"]},\n",
    "    \"min\": {k: 1 for k in [\"accuracy\", \"accuracy_balanced\", \"precision\", \"recall\", \"f1\", \"auc\"]},\n",
    "    \"all\": {k: [] for k in [\"accuracy\", \"accuracy_balanced\", \"precision\", \"recall\", \"f1\", \"auc\"]}\n",
    "  }\n",
    "\n",
    "def show_confusion_matrices(cm, method_name):\n",
    "  cm_np = cp.asnumpy(cm).astype(int)\n",
    "  plt.figure(figsize=(6, 5))\n",
    "  sns.heatmap(cm_np, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "  plt.title(f\"Confusion Matrix of {method_name}\")\n",
    "  plt.xlabel(\"Predicted Label\")\n",
    "  plt.ylabel(\"True Label\")\n",
    "  plt.show()\n",
    "\n",
    "  cm_normalized = cm_np / np.sum(cm_np)\n",
    "  plt.figure(figsize=(6, 5))\n",
    "  sns.heatmap(cm_normalized, annot=True, fmt=\".2%\", cmap=\"Blues\")\n",
    "  plt.title(f\"Normalized Confusion Matrix of {method_name}\")\n",
    "  plt.xlabel(\"Predicted Label\")\n",
    "  plt.ylabel(\"True Label\")\n",
    "  plt.show()\n",
    "\n",
    "def summarize_and_plot(metrics, confusion_matrix_accumulated, method_name, all_fprs, all_tprs):\n",
    "  print(f\"\\n===== Individual Results for {method_name} =====\")\n",
    "  for i in range(len(metrics[\"all\"][\"accuracy\"])):\n",
    "    print(f\"Run {i+1:>2}:\", end=' ')\n",
    "    for metric in metrics[\"all\"]:\n",
    "      print(f\"{metric} = {metrics['all'][metric][i]:.4f}\", end=' | ')\n",
    "    print()\n",
    "\n",
    "  print(f\"\\n===== Summary for {method_name} =====\")\n",
    "  for metric in metrics[\"all\"]:\n",
    "    values = metrics[\"all\"][metric]\n",
    "    mean, std = np.mean(values), np.std(values)\n",
    "    max_val, min_val = metrics[\"max\"][metric], metrics[\"min\"][metric]\n",
    "    print(f\"{metric:<20} Mean = {mean:.4f}, Std = {std:.4f}, Max = {max_val:.4f}, Min = {min_val:.4f}\")\n",
    "\n",
    "  # Draw ROC curve with AUC\n",
    "  mean_fpr = np.linspace(0, 1, 100)\n",
    "  interp_tprs = []\n",
    "  for fpr_i, tpr_i in zip(all_fprs, all_tprs):\n",
    "    interp_func = interp1d(fpr_i, tpr_i, bounds_error=False, fill_value=0)\n",
    "    interp_tpr = interp_func(mean_fpr)\n",
    "    interp_tprs.append(interp_tpr)\n",
    "\n",
    "  mean_tpr = np.mean(interp_tprs, axis=0)\n",
    "  auc_mean = np.mean(metrics[\"all\"][\"auc\"])\n",
    "\n",
    "  plt.figure(figsize=(6, 5))\n",
    "  plt.plot(mean_fpr, mean_tpr, label=f'Mean ROC (AUC = {auc_mean:.4f})')\n",
    "  plt.plot([0, 1], [0, 1], 'r--')\n",
    "  plt.xlabel(\"False Positive Rate\")\n",
    "  plt.ylabel(\"True Positive Rate\")\n",
    "  plt.title(f'Average ROC Curve ({method_name})')\n",
    "  plt.legend(loc=\"lower right\")\n",
    "  plt.grid(True)\n",
    "  plt.show()\n",
    "\n",
    "  # Á¥ØÁ©çÊ∑∑Ê∑ÜÁü©Èô£\n",
    "  cm_acc = cp.asnumpy(confusion_matrix_accumulated).astype(int)\n",
    "  plt.figure(figsize=(6, 5))\n",
    "  sns.heatmap(cm_acc, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "  plt.title(f\"Accumulated Confusion Matrix ({method_name})\")\n",
    "  plt.xlabel(\"Predicted Label\")\n",
    "  plt.ylabel(\"True label\")\n",
    "  plt.show()\n",
    "\n",
    "  cm_normalized = cm_acc / np.sum(cm_acc)\n",
    "  plt.figure(figsize=(6, 5))\n",
    "  sns.heatmap(cm_normalized, annot=True, fmt=\".2%\", cmap=\"Blues\")\n",
    "  plt.title(f\"Normalized Confusion Matrix ({method_name})\")\n",
    "  plt.xlabel(\"Predicted Label\")\n",
    "  plt.ylabel(\"True Label\")\n",
    "  plt.show()\n",
    "\n",
    "def evaluate_and_accumulate(Y_true, Y_pred, Y_prob, accumulators, confusion_matrix_total):\n",
    "  acc = accuracy_score(Y_true, Y_pred)\n",
    "  acc_bal = balanced_accuracy_score(Y_true, Y_pred)\n",
    "  prec = precision_score(Y_true, Y_pred)\n",
    "  rec = recall_score(Y_true, Y_pred)\n",
    "  f1 = f1_score(Y_true, Y_pred)\n",
    "  auc = roc_auc_score(Y_true, Y_prob)\n",
    "  cm = confusion_matrix(Y_true, Y_pred)\n",
    "\n",
    "  for metric, value in zip(\n",
    "    [\"accuracy\", \"accuracy_balanced\", \"precision\", \"recall\", \"f1\", \"auc\"],\n",
    "    [acc, acc_bal, prec, rec, f1, auc]\n",
    "  ):\n",
    "    accumulators[\"max\"][metric] = max(accumulators[\"max\"][metric], value)\n",
    "    accumulators[\"min\"][metric] = min(accumulators[\"min\"][metric], value)\n",
    "    accumulators[\"all\"][metric].append(value)\n",
    "\n",
    "  confusion_matrix_total += cm\n",
    "  return cm\n",
    "\n",
    "def to_numpy(x):\n",
    "  if isinstance(x, cp.ndarray):\n",
    "      return cp.asnumpy(x)\n",
    "  elif isinstance(x, cudf.Series) or isinstance(x, cudf.DataFrame):\n",
    "      return x.to_pandas().values\n",
    "  return x\n",
    "\n",
    "def run_model_evaluation_loop(model_name, X_train, X_test, Y_train, Y_test, run, method_name):\n",
    "  confusion_matrix_accumulated = cp.zeros((2, 2))\n",
    "  metrics = initialize_metrics()\n",
    "  all_fprs = []\n",
    "  all_tprs = []\n",
    "\n",
    "  # select_random_number = random.randint(0, run - 1)\n",
    "\n",
    "  seeds_used = [random.randint(0, 99999) for _ in range(run)]\n",
    "\n",
    "  for i in range(run):\n",
    "    print(f\"Run {i+1}/{run} with seed {seeds_used[i]}\")\n",
    "    # ‰ΩøÁî® SMOTE ÈÄ≤Ë°åÈÅéÊäΩÊ®£\n",
    "    print(\"Applying SMOTE to balance the training set...\")\n",
    "    smote = SMOTE(random_state=seeds_used[i])\n",
    "    X_train_resampled, Y_train_resampled = smote.fit_resample(X_train, Y_train)\n",
    "\n",
    "    X_train_cudf = cudf.DataFrame(X_train_resampled)\n",
    "    Y_train_cudf = cudf.Series(Y_train_resampled)\n",
    "\n",
    "    if model_name == \"LR\":\n",
    "      print('Using Logistic Regression')\n",
    "      model = cuLR()\n",
    "    elif model_name == \"SVM\":\n",
    "      print('Using cuML SVM (linear kernel on GPU)')\n",
    "      model = cuSVC(kernel='linear', probability=True)\n",
    "    elif model_name == \"RF\":\n",
    "      print('Using Random Forest')\n",
    "      model = cuRF()\n",
    "    elif model_name == \"LightGBM\":\n",
    "      print('Using LightGBM')\n",
    "      model = LGBMClassifier()\n",
    "    elif model_name == \"MLP\":\n",
    "      print(\"Using PyTorch MLP (with early stopping)\")\n",
    "      model = MLPClassifier(\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        learning_rate_init=0.001,\n",
    "        max_iter=200,\n",
    "        early_stopping=True, # Not default\n",
    "        random_state=seeds_used[i]\n",
    "      )\n",
    "    else:\n",
    "      raise ValueError(\"Unknown model name\")\n",
    "\n",
    "    print(\"Training...\")\n",
    "    if model_name == \"LightGBM\" or \"MLP\":\n",
    "      model.fit(X_train_resampled, Y_train_resampled)\n",
    "    elif model_name not in [\"MLP\"]:  # MLP don't need to fit\n",
    "      model.fit(X_train_cudf, Y_train_cudf)\n",
    "\n",
    "    print(\"Predicting...\")\n",
    "    Y_preds = model.predict(X_test)\n",
    "    Y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    Y_preds_cpu = to_numpy(Y_preds)\n",
    "    Y_pred_proba_cpu = to_numpy(Y_pred_proba)\n",
    "    print('PROB: ', Y_pred_proba_cpu)\n",
    "    Y_test_cpu = to_numpy(Y_test)\n",
    "\n",
    "    fpr_i, tpr_i, _ = roc_curve(Y_test_cpu, Y_pred_proba_cpu)\n",
    "    all_fprs.append(fpr_i)\n",
    "    all_tprs.append(tpr_i)\n",
    "\n",
    "    cm = evaluate_and_accumulate(Y_test_cpu, Y_preds_cpu, Y_pred_proba_cpu, metrics, confusion_matrix_accumulated)\n",
    "\n",
    "    # if i == select_random_number:\n",
    "    #   print('====== One of running result ======')\n",
    "    #   show_confusion_matrices(cm, method_name)\n",
    "  print('-------------------- Running 10 times Done ---------------------')\n",
    "  summarize_and_plot(metrics, confusion_matrix_accumulated, method_name, all_fprs, all_tprs)\n",
    "\n",
    "def run_models(\n",
    "    X_train_raw, X_test_raw, Y_train, Y_test,\n",
    "    feature_func, run=10, method_name=\"\"\n",
    "):\n",
    "  print('-' * 30)\n",
    "  print(\"Extracting features using \", method_name)\n",
    "  print(f'Sampling Rate: {SR}, with denoising method: {denoising_method}')\n",
    "  \n",
    "  with timed_block(\"Feature Extraction\"):\n",
    "    if feature_func == preprocess_riemann_features:\n",
    "      X_train_feat, X_test_feat = feature_func(X_train_raw, X_test_raw, Y_train)\n",
    "    else:\n",
    "      X_train_feat = feature_func(X_train_raw, sampling_rate=SR)\n",
    "      X_test_feat = feature_func(X_test_raw, sampling_rate=SR)\n",
    "  \n",
    "  scenarios = {\n",
    "    \"no_pca\": {\"X_train\": X_train_feat, \"X_test\": X_test_feat}\n",
    "  }\n",
    "\n",
    "  for pca_mode, data_pair in scenarios.items():\n",
    "    print('-' * 30)\n",
    "    for model_name in [\"SVM\", \"LR\", \"RF\"]: # [\"LR\", \"SVM\", \"RF\", \"LightGBM\", \"KNN\", \"MLP\"]\n",
    "      print(f\"\\n---- Running Model: {model_name} ----\")\n",
    "      with timed_block(f\"{model_name} Ê®°Âûã running time\"):\n",
    "        run_model_evaluation_loop(\n",
    "          model_name=model_name,\n",
    "          X_train=data_pair[\"X_train\"],\n",
    "          X_test=data_pair[\"X_test\"],\n",
    "          Y_train=Y_train,\n",
    "          Y_test=Y_test,\n",
    "          run=run,\n",
    "          method_name=f\"{model_name}_{denoising_method}_XCTS\"\n",
    "        )\n",
    "        print('-' * 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae3174e",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff9e8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data shaping\n",
    "epoch_len = 140 # (IC-U-Net: 136, Others: 140)\n",
    "\n",
    "train_data_shaped = np.reshape(train_data_list,\n",
    "  (train_subj_num * stimulus_per_subj, len(channels), epoch_len))\n",
    "test_data_shaped = np.reshape(test_data_list,\n",
    "  (test_subj_num * stimulus_per_subj, len(channels), epoch_len))\n",
    "\n",
    "Y_train = pd.read_csv('data/TrainLabels.csv')['Prediction'].values\n",
    "\n",
    "Y_test = np.reshape(pd.read_csv('./data/true_labels.csv', header=None).values, 3400)\n",
    "\n",
    "run_models(\n",
    "    X_train_raw=train_data_shaped,\n",
    "    X_test_raw=test_data_shaped,\n",
    "    Y_train=Y_train,\n",
    "    Y_test=Y_test,\n",
    "    feature_func=preprocess_riemann_features,\n",
    "    run=10,\n",
    "    method_name=\"XCTS\"\n",
    ")\n",
    "\n",
    "print('-' * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
