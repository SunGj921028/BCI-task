{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cc9f384",
   "metadata": {},
   "source": [
    "## Import some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f79a9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                                      # for dealing with data\n",
    "import pandas as pd\n",
    "from scipy.signal import butter, sosfiltfilt, sosfreqz  # for filtering\n",
    "import matplotlib.pyplot as plt                         # for plotting\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix, roc_curve, auc\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join, isdir\n",
    "\n",
    "# For IC-U-Net\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Permute, Dropout\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from tensorflow.keras.layers import SeparableConv2D, DepthwiseConv2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import SpatialDropout2D\n",
    "from tensorflow.keras.layers import Input, Flatten\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import utils as np_utils\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# For ASR\n",
    "import mne\n",
    "import asrpy\n",
    "from asrpy import asr_calibrate, asr_process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e58ad5",
   "metadata": {},
   "source": [
    "## Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b3f8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 200.0     # 200 Hz sampling rate\n",
    "lowcut = 1.0   \n",
    "highcut = 50.0\n",
    "\n",
    "epoch_s = -100      # epoch starting time relative to stmulus in miliseconds\n",
    "epoch_e = 600    # epoch ending time relative to stmulus in miliseconds\n",
    "bl_s = -100         # baseline starting time relative to stmulus in miliseconds\n",
    "bl_e = 0       # baseline ending time relative to stmulus in miliseconds\n",
    "epoch_len = int((abs(epoch_s) + abs(epoch_e)) * (fs / 1000))\n",
    "print(epoch_len)\n",
    "\n",
    "train_subj_num = 16\n",
    "test_subj_num = 10\n",
    "stimulus_per_subj = 340\n",
    "trial_per_subj = 5\n",
    "\n",
    "channels = [\n",
    "    'Fp1', 'Fp2', 'F7', 'F3', 'Fz', 'F4', 'F8',\n",
    "    'FT7', 'FC3', 'FCz', 'FC4', 'FT8',\n",
    "    'T7', 'C3', 'Cz', 'C4', 'T8',\n",
    "    'TP7', 'CP3', 'CPz', 'CP4', 'TP8',\n",
    "    'P7', 'P3', 'Pz', 'P4', 'P8',\n",
    "    'O1', 'POz', 'O2'\n",
    "]\n",
    "print(len(channels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e0dfc1",
   "metadata": {},
   "source": [
    "## Bandpass Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb96fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For butterworth band pass filter\n",
    "def butter_bandpass_filter(raw_data, fs, lowcut = 1.0, highcut = 50.0, order = 5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    sos = butter(order, [low, high], analog = False, btype = 'band', output = 'sos')\n",
    "    filted_data = sosfiltfilt(sos, raw_data)\n",
    "    return filted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d66347",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6d9d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv(\"./data/TrainLabels.csv\") #! Training's labels（ground truth）, 5440\n",
    "\n",
    "train_list_arr = np.array(sorted(listdir('./data/train')))\n",
    "train_list_np = np.reshape(\n",
    "    train_list_arr, (train_subj_num, trial_per_subj)) #! (16, 5)\n",
    "\n",
    "test_list_arr = np.array(sorted(listdir('./data/test')))\n",
    "test_list_np = np.reshape(\n",
    "    test_list_arr, (test_subj_num, trial_per_subj)) #! (10, 5)\n",
    "\n",
    "print(train_list_np.shape, test_list_np.shape)\n",
    "\n",
    "train_data_list = np.empty(\n",
    "    (0, stimulus_per_subj, len(channels), epoch_len), np.float64) #! (0, 340, 30, 140) or (0, 340, 30, 156)\n",
    "test_data_list = np.empty(\n",
    "    (0, stimulus_per_subj, len(channels), epoch_len), np.float64) #! (0, 340, 30, 140) or (0, 340, 30, 156)\n",
    "\n",
    "print(train_data_list.shape, test_data_list.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a84055d",
   "metadata": {},
   "source": [
    "## IC-U-Net (need model/ and the .pth file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35299f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import time\n",
    "import torch\n",
    "import os\n",
    "import shutil\n",
    "from scipy.signal import decimate, resample_poly, firwin, lfilter\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "class VGGBlock(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channels, out_channels, ks=7):\n",
    "        super().__init__()\n",
    "        padding = int((ks - 1) / 2)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv1d(in_channels, middle_channels, kernel_size=ks, padding=padding)\n",
    "        self.bn1 = nn.BatchNorm1d(middle_channels)\n",
    "        self.conv2 = nn.Conv1d(middle_channels, out_channels, kernel_size=ks, padding=padding)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class NestedUNet3(nn.Module):\n",
    "    def __init__(self, num_classes, input_channels=30, deep_supervision=False, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        nb_filter = [32, 64, 128, 256]\n",
    "\n",
    "        self.deep_supervision = deep_supervision\n",
    "\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        # self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='linear', align_corners=False)\n",
    "\n",
    "        self.conv0_0 = VGGBlock(input_channels, nb_filter[0], nb_filter[0])\n",
    "        self.conv1_0 = VGGBlock(nb_filter[0], nb_filter[1], nb_filter[1])\n",
    "        self.conv2_0 = VGGBlock(nb_filter[1], nb_filter[2], nb_filter[2])\n",
    "        self.conv3_0 = VGGBlock(nb_filter[2], nb_filter[3], nb_filter[3])\n",
    "\n",
    "        self.conv0_1 = VGGBlock(nb_filter[0]+nb_filter[1], nb_filter[0], nb_filter[0])\n",
    "        self.conv1_1 = VGGBlock(nb_filter[1]+nb_filter[2], nb_filter[1], nb_filter[1])\n",
    "        self.conv2_1 = VGGBlock(nb_filter[2]+nb_filter[3], nb_filter[2], nb_filter[2])\n",
    "\n",
    "        self.conv0_2 = VGGBlock(nb_filter[0]*2+nb_filter[1], nb_filter[0], nb_filter[0])\n",
    "        self.conv1_2 = VGGBlock(nb_filter[1]*2+nb_filter[2], nb_filter[1], nb_filter[1])\n",
    "\n",
    "        self.conv0_3 = VGGBlock(nb_filter[0]*3+nb_filter[1], nb_filter[0], nb_filter[0])\n",
    "\n",
    "        #if self.deep_supervision:\n",
    "        self.final1 = nn.Conv1d(nb_filter[0], num_classes, kernel_size=1)\n",
    "        self.final2 = nn.Conv1d(nb_filter[0], num_classes, kernel_size=1)\n",
    "        self.final3 = nn.Conv1d(nb_filter[0], num_classes, kernel_size=1)\n",
    "        #else:\n",
    "            #self.final = nn.Conv1d(nb_filter[0], num_classes, kernel_size=1)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        x0_0 = self.conv0_0(input)\n",
    "        x1_0 = self.conv1_0(self.pool(x0_0))\n",
    "        x0_1 = self.conv0_1(torch.cat([x0_0, self.up(x1_0)], 1))\n",
    "        #print(\"input:\", input.shape)\n",
    "        #print(\"x0_0: \", x0_0.shape)\n",
    "        #print(\"pool: \", self.pool(x0_0).shape)\n",
    "        #print(\"x1_0: \", x1_0.shape)\n",
    "        #print(\"up:   \", self.up(x1_0).shape)\n",
    "        #print(\"cat:  \", torch.cat([x0_0, self.up(x1_0)], 1).shape)\n",
    "        #print(\"x0_1: \", x0_1.shape)\n",
    "\n",
    "        x2_0 = self.conv2_0(self.pool(x1_0))\n",
    "        x1_1 = self.conv1_1(torch.cat([x1_0, self.up(x2_0)], 1))\n",
    "        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.up(x1_1)], 1))\n",
    "\n",
    "        x3_0 = self.conv3_0(self.pool(x2_0))\n",
    "        x2_1 = self.conv2_1(torch.cat([x2_0, self.up(x3_0)], 1))\n",
    "        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.up(x2_1)], 1))\n",
    "        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.up(x1_2)], 1))\n",
    "\n",
    "        #if self.deep_supervision:\n",
    "        output1 = self.final1(x0_1)\n",
    "        output2 = self.final2(x0_2)\n",
    "        output3 = self.final3(x0_3)\n",
    "        return output1, output2, output3\n",
    "        \"\"\"\n",
    "        else:\n",
    "            output = self.final(x0_4)\n",
    "            return output\n",
    "        \"\"\"\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels,kernel_size=7):\n",
    "        super().__init__()\n",
    "        padding = int((kernel_size - 1) / 2)\n",
    "\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.Sigmoid(),\n",
    "            #nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            #nn.ReLU(inplace=True)\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool1d(2),\n",
    "            DoubleConv(in_channels, out_channels,kernel_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            # self.up = F.interpolate()\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='linear', align_corners=False)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose1d(in_channels // 2, in_channels // 2, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv = DoubleConv(in_channels, out_channels, kernel_size)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x = self.up(x1)\n",
    "        # input is CHW\n",
    "        #diffY = torch.tensor([x2.size()[2] - x1.size()[2]])\n",
    "        #diffX = torch.tensor([x2.size()[3] - x1.size()[3]])\n",
    "\n",
    "        #x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "        #                diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        #x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        super(OutConv, self).__init__()\n",
    "        padding = int((kernel_size - 1) / 2)\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=padding, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet1(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(UNet1, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64, kernel_size=7)\n",
    "        self.down1 = Down(64, 128, kernel_size=7)\n",
    "        self.down2 = Down(128, 256,kernel_size=5)\n",
    "        self.down3 = Down(256, 512,kernel_size=3)\n",
    "        self.up1 = Up(512, 256, kernel_size=3)\n",
    "        self.up2 = Up(256, 128, kernel_size=3)\n",
    "        self.up3 = Up(128, 64, kernel_size=3)\n",
    "        self.outc = OutConv(64, n_classes,kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x = self.up1(x4, x3)\n",
    "        x = self.up2(x, x2)\n",
    "        x = self.up3(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "\n",
    "def resample(signal, fs):\n",
    "    # downsample the signal to a sample rate of 256 Hz\n",
    "    if fs>256:\n",
    "        fs_down = 256 # Desired sample rate\n",
    "        q = int(fs / fs_down) # Downsampling factor\n",
    "        signal_new = []\n",
    "        for ch in signal:\n",
    "            x_down = decimate(ch, q)\n",
    "            signal_new.append(x_down)\n",
    "\n",
    "    # upsample the signal to a sample rate of 256 Hz\n",
    "    elif fs<256:\n",
    "        fs_up = 256  # Desired sample rate\n",
    "        p = int(fs_up / fs)  # Upsampling factor \n",
    "        signal_new = []\n",
    "        for ch in signal:\n",
    "            x_up = resample_poly(ch, p, 1)\n",
    "            signal_new.append(x_up)\n",
    "\n",
    "    else:\n",
    "        signal_new = signal\n",
    "\n",
    "    signal_new = np.array(signal_new).astype(np.float64)\n",
    "\n",
    "    return signal_new\n",
    "\n",
    "def FIR_filter(signal, lowcut, highcut):\n",
    "    fs = 256.0\n",
    "    # Number of FIR filter taps\n",
    "    numtaps = 1000\n",
    "    # Use firwin to create a bandpass FIR filter\n",
    "    fir_coeff = firwin(numtaps, [lowcut, highcut], pass_zero=False, fs=fs)\n",
    "    # Apply the filter to signal:\n",
    "    filtered_signal  = lfilter(fir_coeff, 1.0, signal)\n",
    "    \n",
    "    return filtered_signal\n",
    "\n",
    "\n",
    "def read_train_data(file_name):\n",
    "    with open(file_name, 'r', newline='') as f:\n",
    "        lines = csv.reader(f)\n",
    "        data = []\n",
    "        for line in lines:\n",
    "            data.append(line)\n",
    "\n",
    "    data = np.array(data).astype(np.float64)\n",
    "    return data\n",
    "\n",
    "\n",
    "def cut_data(raw_data):\n",
    "    raw_data = np.array(raw_data).astype(np.float64)\n",
    "    total = int(len(raw_data[0]) / 1024)\n",
    "    if total == 0:\n",
    "        total = 1\n",
    "    for i in range(total):\n",
    "        if total == 1:\n",
    "            table = raw_data\n",
    "        else:\n",
    "            table = raw_data[:, i * 1024:(i + 1) * 1024]\n",
    "        filename = './temp2/' + str(i) + '.csv'\n",
    "        with open(filename, 'w', newline='') as csvfile:\n",
    "            print('Writing file {}'.format(filename))\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerows(table)\n",
    "    return total\n",
    "\n",
    "\n",
    "def glue_data(file_name, total):\n",
    "    gluedata = 0\n",
    "    for i in range(total):\n",
    "        file_name1 = file_name + 'output{}.csv'.format(str(i))\n",
    "        with open(file_name1, 'r', newline='') as f:\n",
    "            lines = csv.reader(f)\n",
    "            raw_data = []\n",
    "            for line in lines:\n",
    "                raw_data.append(line)\n",
    "        raw_data = np.array(raw_data).astype(np.float64)\n",
    "        if i == 0:\n",
    "            gluedata = raw_data\n",
    "        else:\n",
    "            smooth = (gluedata[:, -1] + raw_data[:, 1]) / 2\n",
    "            gluedata[:, -1] = smooth\n",
    "            raw_data[:, 1] = smooth\n",
    "            gluedata = np.append(gluedata, raw_data, axis=1)\n",
    "    return gluedata\n",
    "\n",
    "\n",
    "def save_data(data, filename):\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(data)\n",
    "\n",
    "def dataDelete(path):\n",
    "    try:\n",
    "        shutil.rmtree(path)\n",
    "    except OSError as e:\n",
    "        print(e)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "def decode_data(data, std_num, mode=5):\n",
    "\n",
    "    if mode == \"ICUNet\":\n",
    "        model = UNet1(n_channels=30, n_classes=30)\n",
    "        resumeLoc = './model/ICUNet/modelsave' + '/BEST_checkpoint.pth.tar'\n",
    "\n",
    "    elif mode == \"UNetpp\":\n",
    "        model = NestedUNet3(num_classes=30)\n",
    "        resumeLoc = './model/UNetpp/modelsave' + '/checkpoint.pth.tar'\n",
    "\n",
    "    checkpoint = torch.load(resumeLoc, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['state_dict'],False)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # run the mdoel\n",
    "        data = data[np.newaxis, :, :]\n",
    "        data = torch.Tensor(data)\n",
    "        if mode == \"UNetpp\" or mode == \"UNetpp_block\" or mode == \"Trans\" or mode == \"Trans_block\":\n",
    "            decode1, decode2, decode = model(data)\n",
    "        else:\n",
    "            decode = model(data)\n",
    "        if int(std_num) != 0:\n",
    "            decode = decode * std_num\n",
    "    decode = np.array(decode.cpu()).astype(np.float64)\n",
    "    return decode\n",
    "\n",
    "def preprocessing(signal, samplerate):\n",
    "    # establish temp folder\n",
    "    if not os.path.exists(\"./temp2/\"):\n",
    "        os.makedirs(\"./temp2/\", exist_ok=True)\n",
    "    else :\n",
    "        dataDelete(\"./temp2/\")\n",
    "        os.makedirs(\"./temp2/\", exist_ok=True)\n",
    "\n",
    "    # resample\n",
    "    signal = resample(signal, samplerate)\n",
    "    # FIR_filter\n",
    "    signal = FIR_filter(signal, 1, 40) #! In original code highcut = 50\n",
    "    # cutting data\n",
    "    total_file_num = cut_data(signal)\n",
    "\n",
    "    return total_file_num\n",
    "\n",
    "\n",
    "# model = tf.keras.models.load_model('./denoise_model/')\n",
    "def reconstruct(model_name, total):\n",
    "    # -------------------decode_data---------------------------\n",
    "    second1 = time.time()\n",
    "    for i in range(total):\n",
    "        file_name = './temp2/{}.csv'.format(str(i))\n",
    "        data_noise = read_train_data(file_name)\n",
    "\n",
    "        std = np.std(data_noise)\n",
    "        avg = np.average(data_noise)\n",
    "\n",
    "        data_noise = (data_noise-avg)/std\n",
    "\n",
    "        # UNet\n",
    "        d_data = decode_data(data_noise, std, model_name)\n",
    "        d_data = d_data[0]\n",
    "\n",
    "        outputname = \"./temp2/output{}.csv\".format(str(i))\n",
    "        save_data(d_data, outputname)\n",
    "\n",
    "    # --------------------glue_data----------------------------\n",
    "    data_after_denoised = glue_data(\"./temp2/\", total)\n",
    "    # -------------------delete_data---------------------------\n",
    "    dataDelete(\"./temp2/\")\n",
    "    second2 = time.time()\n",
    "\n",
    "    print(\"Reconstruct has been success \", second2 - second1, \"sec(s)\")\n",
    "    return data_after_denoised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e815fe",
   "metadata": {},
   "source": [
    "## Set up denoising type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f10910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# denoise_type = \"\"\n",
    "# denoise_type = \"ASR\"\n",
    "# denoise_type = \"filter\"\n",
    "# denoise_type = \"ICUNet\"\n",
    "denoise_type = \"ICA\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b039c75",
   "metadata": {},
   "source": [
    "## Epoch Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741116f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_epoch(file_path, channels, fs, eeg_filter, stimulus_times=None, baseline=True,  epoch_s=-100, epoch_e=600, bl_s=-100, bl_e=0):\n",
    "    # read dataand data selection\n",
    "    train_data = pd.read_csv(file_path)\n",
    "    \n",
    "    # Use Pandas to read EEG data and convert the time column (assumed to be in seconds) to milliseconds.\n",
    "    # Extract the specified EEG channel data and convert it to NumPy format.\n",
    "    train_data['Time'] *= 1000\n",
    "    \n",
    "    raw_eeg = train_data[channels].values.T.astype(np.float64)\n",
    "    print('EEG shape after channel selection', raw_eeg.shape)\n",
    "    \n",
    "    # Get the index of the stimulus\n",
    "    # According to the data in the FeedBackEvent column equal to 1, determine the index of the event occurrence\n",
    "    train_data['index'] = train_data.index.values\n",
    "    if stimulus_times is None:\n",
    "        mark_indices = train_data[train_data['FeedBackEvent'] == 1].index.to_numpy()\n",
    "    else:\n",
    "        mark_indices = np.round(np.asarray(\n",
    "            stimulus_times).flatten() * fs).astype(int)\n",
    "    \n",
    "    # Determine the length of the epoch\n",
    "    # b_s and b_e: baseline correction window start and end index.\n",
    "    # epoch_len: the total length of each epoch (in sample points).\n",
    "    # e_s and e_e: relative start and end index of the epoch.\n",
    "    b_s = int((abs(epoch_s) + bl_s) * (fs / 1000)) # 0\n",
    "    b_e = int((abs(epoch_s) + bl_e) * (fs / 1000)) # 100\n",
    "    epoch_len = int((abs(epoch_s) + abs(epoch_e)) * (fs / 1000)) # 140 or 136\n",
    "    print('epoch_len: ', epoch_len)\n",
    "    \n",
    "    # Because each time is 5ms, the length of idx needs to be converted to 700 / 5 = 140\n",
    "    e_s = int((epoch_s * (fs / 1000))) # -20\n",
    "    e_e = int((epoch_e * (fs / 1000))) # 120\n",
    "    \n",
    "    # Initialize an empty 3D matrix to store the epoch data for all channels\n",
    "    # Dimension: (number of events, time length, number of channels)\n",
    "    final_epoch = np.zeros((len(mark_indices), len(channels), 140), dtype=np.float64)\n",
    "    final_epoch_after_ICUNet = np.zeros((len(mark_indices), len(channels), 136), dtype=np.float64)\n",
    "    final_epoch_after_asr = np.zeros((len(mark_indices), len(channels), 140), dtype=np.float64)\n",
    "    \n",
    "    if denoise_type == \"ICUNet\":\n",
    "        print('Denoising By ICUNet...')\n",
    "    elif denoise_type == \"ASR\":\n",
    "        print('Denoising By ASR...')\n",
    "    elif denoise_type == \"filter\":\n",
    "        print('Filtering only...')\n",
    "    else:\n",
    "        print('No any denoising method...')\n",
    "    \n",
    "    # Iterate over each event\n",
    "    for i, mark_idx in enumerate(mark_indices):\n",
    "        # Extract the epoch for each channel\n",
    "        for j, channel in enumerate(channels):\n",
    "            rawEEG = train_data[channel].values.astype(np.float64)\n",
    "            epoch = rawEEG[mark_idx + e_s: mark_idx + e_e]  # Extract the data segment of the event\n",
    "            \n",
    "            # If the length is not enough, skip\n",
    "            if len(epoch) != epoch_len:\n",
    "                print('Epoch length not match, skip')\n",
    "                continue\n",
    "            \n",
    "            # Do the denoising\n",
    "            if denoise_type == \"filter\":\n",
    "                filtered_epoch = eeg_filter(epoch, fs, 1, 50)  # For one of the denoising method setting\n",
    "            else:\n",
    "                filtered_epoch = epoch\n",
    "            \n",
    "            # Baseline correction (if enabled)\n",
    "            if baseline:\n",
    "                baseline_mean = np.mean(filtered_epoch[b_s:b_e])\n",
    "                filtered_epoch -= baseline_mean\n",
    "            \n",
    "            # Save the data to the matrix\n",
    "            final_epoch[i, j, :] = filtered_epoch\n",
    "        \n",
    "        if denoise_type == \"ICUNet\":\n",
    "            total_file_num = preprocessing(final_epoch[i, :, :], fs)\n",
    "            # final_epoch_after_ICUNet[i, :, :] = reconstruct(\"UNetpp\", total_file_num)\n",
    "            final_epoch_after_ICUNet[i, :, :] = reconstruct(\"ICUNet\", total_file_num)\n",
    "            print('final_epoch_after_ICUNet shape: ', final_epoch_after_ICUNet[i, :, :].shape)\n",
    "        elif denoise_type == \"ASR\":\n",
    "            M, T = asr_calibrate(final_epoch[i, :, :], fs, cutoff=5)\n",
    "            final_epoch_after_asr[i, :, :] = asr_process(final_epoch[i, :, :], fs , M, T)\n",
    "            \n",
    "    if denoise_type == \"ICUNet\":\n",
    "        return final_epoch_after_ICUNet\n",
    "    elif denoise_type == \"ASR\":\n",
    "        return final_epoch_after_asr\n",
    "    else:\n",
    "        return final_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb91946",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ac926e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last shape: (16, 340, 30, 140) or (16, 340, 30, 136)\n",
    "train_data_list = np.empty((0, stimulus_per_subj, len(channels), epoch_len), np.float64)\n",
    "train_data_list_ICUNet = np.empty((0, stimulus_per_subj, len(channels), 136), np.float64)\n",
    "\n",
    "# Preparing training data\n",
    "if not isfile(\"./data/train_data.npy\"):\n",
    "    for training_participant_id in range(train_subj_num): # 0-15\n",
    "        subject_dir_list = train_list_np[training_participant_id]\n",
    "        subject_epoch = np.empty((0, len(channels), epoch_len), np.float64) # (0, 30, 140)\n",
    "        subject_epoch_ICUNet = np.empty((0, len(channels), 136), np.float64) # (0, 30, 136)\n",
    "\n",
    "        for trial_id in range(trial_per_subj): # 0-4\n",
    "            subject_dir = subject_dir_list[trial_id]\n",
    "            print('Subject directory: ', subject_dir)\n",
    "            \n",
    "            data = generate_epoch('./data/train/'+subject_dir, channels, fs,\n",
    "                    butter_bandpass_filter, epoch_s = epoch_s, epoch_e = epoch_e, bl_s = bl_s, bl_e = bl_e)\n",
    "            \n",
    "            print('Epoched data shape: ', data.shape)\n",
    "            if denoise_type == \"ICUNet\":\n",
    "                subject_epoch_ICUNet = np.vstack((subject_epoch_ICUNet, data)) # Last shape: (340, 30, 136)\n",
    "            else:\n",
    "                subject_epoch = np.vstack((subject_epoch, data)) # Last shape: (340, 30, 140)\n",
    "\n",
    "        if denoise_type == \"ICUNet\":\n",
    "            subject_epoch_ICUNet = np.expand_dims(subject_epoch_ICUNet, axis=0)\n",
    "        else:\n",
    "            subject_epoch = np.expand_dims(subject_epoch, axis=0)\n",
    "        \n",
    "        if denoise_type == \"ICUNet\":\n",
    "            print('Epoched subject data shape: ' + str(subject_epoch_ICUNet.shape))\n",
    "        else:\n",
    "            print('Epoched subject data shape: ' + str(subject_epoch.shape))\n",
    "        \n",
    "        # (16, 340, 30, 140) all subjects' data\n",
    "        if denoise_type == \"ICUNet\":\n",
    "            train_data_list_ICUNet = np.vstack((train_data_list_ICUNet, subject_epoch_ICUNet))\n",
    "        else:\n",
    "            train_data_list = np.vstack((train_data_list, subject_epoch))\n",
    "\n",
    "    # Store the data after denoising\n",
    "    if denoise_type == \"ICUNet\":\n",
    "        print('Train data list denoised: ', train_data_list_ICUNet.shape)\n",
    "        np.save('./data/train_data.npy', train_data_list_ICUNet)\n",
    "    else:\n",
    "        print('Train data list: ', train_data_list.shape)\n",
    "        np.save('./data/train_data.npy', train_data_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e2020a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing testing data\n",
    "\n",
    "# Last shape: (16, 340, 30, 140) or (16, 340, 30, 136)\n",
    "test_data_list = np.empty((0, stimulus_per_subj, len(channels), epoch_len), np.float64)\n",
    "test_data_list_ICUNet = np.empty((0, stimulus_per_subj, len(channels), 136), np.float64)\n",
    "\n",
    "if not isfile(\"./data/test_data.npy\"):\n",
    "    for testing_participant_id in range(test_subj_num): # 0-15\n",
    "        subject_dir_list = test_list_np[testing_participant_id]\n",
    "        subject_epoch = np.empty((0, len(channels), epoch_len), np.float64) # (0, 30, 140)\n",
    "        subject_epoch_denoised = np.empty((0, len(channels), 136), np.float64) # (0, 30, 136)\n",
    "\n",
    "        for trial_id in range(trial_per_subj): # 0-4\n",
    "            subject_dir = subject_dir_list[trial_id]\n",
    "            print('Subject directory: ', subject_dir)\n",
    "            \n",
    "            data = generate_epoch('./data/test/'+subject_dir, channels, fs,\n",
    "                butter_bandpass_filter, baseline=True, epoch_s = epoch_s, epoch_e = epoch_e, bl_s = bl_s, bl_e = bl_e)\n",
    "            \n",
    "            print('Epoched data shape: ', data.shape)\n",
    "            if denoise_type == \"ICUNet\":\n",
    "                subject_epoch_denoised = np.vstack((subject_epoch_denoised, data)) # Last shape: (340, 30, 136)\n",
    "            else:\n",
    "                subject_epoch = np.vstack((subject_epoch, data)) # Last shape: (340, 30, 140)\n",
    "\n",
    "        if denoise_type == \"ICUNet\":\n",
    "            subject_epoch_denoised = np.expand_dims(subject_epoch_denoised, axis=0)\n",
    "        else:\n",
    "            subject_epoch = np.expand_dims(subject_epoch, axis=0)\n",
    "        \n",
    "        if denoise_type == \"ICUNet\":\n",
    "            print('Epoched subject data shape: ' + str(subject_epoch_denoised.shape))\n",
    "        else:\n",
    "            print('Epoched subject data shape: ' + str(subject_epoch.shape))\n",
    "        \n",
    "        if denoise_type == \"ICUNet\":\n",
    "            test_data_list_ICUNet = np.vstack((test_data_list_ICUNet, subject_epoch_denoised))\n",
    "        else:\n",
    "            test_data_list = np.vstack((test_data_list, subject_epoch)) # Last shape: (16, 340, 30, 140)\n",
    "\n",
    "    if denoise_type == \"ICUNet\":\n",
    "        print('test data list denoised: ', test_data_list_ICUNet.shape)\n",
    "        np.save('./data/test_data.npy', test_data_list_ICUNet)\n",
    "    else:\n",
    "        print('test data list: ', test_data_list.shape)\n",
    "        np.save('./data/test_data.npy', test_data_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
