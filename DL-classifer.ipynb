{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3c1522a",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a112e38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np                                      # for dealing with data\n",
    "from scipy.signal import butter, sosfiltfilt, sosfreqz  # for filtering\n",
    "import matplotlib.pyplot as plt                         # for plotting\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join, isdir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effdbc26",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa24e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_list = np.load('./data/train_data.npy')\n",
    "test_data_list = np.load('./data/test_data.npy')\n",
    "print('Epoched training data shape: ' + str(train_data_list.shape)) #! (16, 340, 56, 140)\n",
    "print('Epoched training data: ' , train_data_list)\n",
    "print('Epoched testing data shape: ' + str(test_data_list.shape)) #! (10, 340, 56, 140)\n",
    "print('Epoched testing data: ' , test_data_list)\n",
    "Y_true_Labels_for_test = np.reshape(pd.read_csv('./data/true_labels.csv', header=None).values, 3400)\n",
    "# Shape: (3400,)\n",
    "print('Y_true_Labels_for_test: ', Y_true_Labels_for_test.shape)\n",
    "unique, counts = np.unique(Y_true_Labels_for_test, return_counts=True)\n",
    "print(\"Class distribution:\")\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"Class {label}: {count}\")\n",
    "    \n",
    "subj, trial, numChannel, sample = train_data_list.shape #! (16, 340, 30, 140)\n",
    "X_train_valid = np.reshape(train_data_list, (-1, 1, numChannel, sample)) #! (5440, 1, 30, 140)\n",
    "X_test = np.reshape(test_data_list, (-1, 1, numChannel, sample)) #! (3400, 1, 30, 140)\n",
    "\n",
    "Y_train_valid = pd.read_csv('data/TrainLabels.csv')['Prediction'].values\n",
    "\n",
    "print('subject: ', subj)\n",
    "print('trial: ', trial)\n",
    "print('numChannel: ', numChannel)\n",
    "print('sample: ', sample)\n",
    "print('X_train_valid: ', X_train_valid.shape)\n",
    "print('X_test: ', X_test.shape)\n",
    "print('Y_train_valid: ', Y_train_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c4fdb5",
   "metadata": {},
   "source": [
    "## EEGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfa1b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.layers import Conv2D, AveragePooling2D\n",
    "from tensorflow.keras.layers import SeparableConv2D, DepthwiseConv2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import SpatialDropout2D\n",
    "from tensorflow.keras.layers import Input, Flatten\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "\n",
    "def EEGNet(nb_classes, Chans = 56, Samples = 128, \n",
    "             dropoutRate = 0.5, kernLength = 64, F1 = 8, \n",
    "             D = 2, F2 = 16, norm_rate = 0.25, dropoutType = 'Dropout'):\n",
    "    if dropoutType == 'SpatialDropout2D':\n",
    "        dropoutType = SpatialDropout2D\n",
    "    elif dropoutType == 'Dropout':\n",
    "        dropoutType = Dropout\n",
    "    else:\n",
    "        raise ValueError('dropoutType must be one of SpatialDropout2D '\n",
    "                         'or Dropout, passed as a string.')\n",
    "            \n",
    "    input1   = Input(shape = (1, Chans, Samples))\n",
    "    \n",
    "    ##################################################################\n",
    "    block1       = Conv2D(F1, (1, kernLength), padding = 'same',\n",
    "                                   input_shape = (1, Chans, Samples),\n",
    "                                   use_bias = False)(input1)\n",
    "    block1       = BatchNormalization(axis = 1)(block1)\n",
    "    block1       = DepthwiseConv2D((Chans, 1), use_bias = False, \n",
    "                                   depth_multiplier = D,\n",
    "                                   depthwise_constraint = max_norm(1.),\n",
    "                                  data_format='channels_first')(block1)\n",
    "    block1       = BatchNormalization(axis = 1)(block1)\n",
    "    block1       = Activation('elu')(block1)\n",
    "    block1       = AveragePooling2D((1, 4), data_format='channels_first')(block1)\n",
    "    block1       = dropoutType(dropoutRate)(block1)\n",
    "    \n",
    "    block2       = SeparableConv2D(F2, (1, 16),\n",
    "                                   use_bias = False, padding = 'same')(block1)\n",
    "    block2       = BatchNormalization(axis = 1)(block2)\n",
    "    block2       = Activation('elu')(block2)\n",
    "    block2       = AveragePooling2D((1, 8), data_format='channels_first')(block2)\n",
    "    block2       = dropoutType(dropoutRate)(block2)\n",
    "        \n",
    "    flatten      = Flatten(name = 'flatten')(block2)\n",
    "    \n",
    "    dense        = Dense(nb_classes, name = 'dense', \n",
    "                         kernel_constraint = max_norm(norm_rate))(flatten)\n",
    "    softmax      = Activation('softmax', name = 'softmax')(dense)\n",
    "    \n",
    "    return Model(inputs=input1, outputs=softmax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99765e89",
   "metadata": {},
   "source": [
    "## z-score normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ebd2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def z_score_normalize(X):\n",
    "    mean = np.mean(X, axis=-1, keepdims=True)\n",
    "    std = np.std(X, axis=-1, keepdims=True)\n",
    "    return (X - mean) / (std + 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071d6c86",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f4d6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.metrics import f1_score, balanced_accuracy_score, accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define lists to store results\n",
    "accuracy_results = []\n",
    "accuracy_results_balanced = []\n",
    "precision_results = []\n",
    "recall_results = []\n",
    "f1_results = []\n",
    "auc_result = []\n",
    "\n",
    "# Number of model runs\n",
    "n_runs = 10\n",
    "\n",
    "# Z-score normalization on the full training+validation data\n",
    "X_train_valid = z_score_normalize(X_train_valid)\n",
    "# Normalize the test set (separately!)\n",
    "X_test = z_score_normalize(X_test)\n",
    "\n",
    "for run in range(n_runs):\n",
    "    print(f\"Running iteration {run + 1}/{n_runs}...\")\n",
    "    \n",
    "    # Re-split training/validation set (ensure different random allocation)\n",
    "    print(X_train_valid.shape, Y_train_valid.shape)\n",
    "    X_train, X_valid, Y_train, Y_valid = train_test_split(\n",
    "        X_train_valid, Y_train_valid, test_size=0.25, stratify=Y_train_valid, random_state=run\n",
    "    )\n",
    "\n",
    "    # Calculate class_weights\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced', classes=np.unique(Y_train), y=Y_train\n",
    "    )\n",
    "    class_weights = dict(enumerate(class_weights))  # Convert to dictionary format\n",
    "    print(\"Class Weights:\", class_weights)\n",
    "\n",
    "    # Initialize and train model\n",
    "    model = EEGNet(nb_classes=2, Chans=numChannel, Samples=sample, dropoutRate=0.5)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # Set ModelCheckpoint\n",
    "    checkpointer = ModelCheckpoint(filepath=f'/tmp/checkpoint_{run}.keras', verbose=0, save_best_only=True, monitor='val_loss', mode='min')\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True, verbose=1)\n",
    "    \n",
    "    # Define ReduceLROnPlateau\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',       # Monitor the metric\n",
    "        factor=0.3,               # Learning rate decrease factor\n",
    "        patience=10,              # Number of epochs to wait before reducing learning rate\n",
    "        min_lr=1e-6,              # Minimum learning rate\n",
    "        verbose=1                 # Show messages\n",
    "    )\n",
    "    \n",
    "    fittedModel = model.fit(X_train, Y_train, batch_size=64, epochs=150, verbose=2,\n",
    "            validation_data=(X_valid, Y_valid),\n",
    "            callbacks=[checkpointer, early_stopping], class_weight=class_weights)\n",
    "\n",
    "    # Load best weights\n",
    "    model.load_weights(f'/tmp/checkpoint_{run}.keras')\n",
    "\n",
    "    # Output layer: softmax\n",
    "    print(X_test.shape, Y_true_Labels_for_test.shape) \n",
    "    y_probs = model.predict(X_test)[:, 1]  # ÂèñÂæóÈ°ûÂà• 1 ÁöÑÊ©üÁéá\n",
    "    y_pred = model.predict(X_test).argmax(axis=-1)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(Y_true_Labels_for_test, y_pred)\n",
    "    accuracy_balanced = balanced_accuracy_score(Y_true_Labels_for_test, y_pred)\n",
    "    precision = precision_score(Y_true_Labels_for_test, y_pred)\n",
    "    recall = recall_score(Y_true_Labels_for_test, y_pred)\n",
    "    f1 = f1_score(Y_true_Labels_for_test, y_pred)\n",
    "    auc = roc_auc_score(Y_true_Labels_for_test, y_probs)  # Use probability of positive class\n",
    "    \n",
    "    # Save results\n",
    "    accuracy_results.append(accuracy)\n",
    "    accuracy_results_balanced.append(accuracy_balanced)\n",
    "    precision_results.append(precision)\n",
    "    recall_results.append(recall)\n",
    "    f1_results.append(f1)\n",
    "    auc_result.append(auc)\n",
    "\n",
    "# Calculate mean and std\n",
    "accuracy_mean, accuracy_std = np.mean(accuracy_results), np.std(accuracy_results)\n",
    "accuracy_mean_balanced, accuracy_std_balanced = np.mean(accuracy_results_balanced), np.std(accuracy_results_balanced)\n",
    "precision_mean, precision_std = np.mean(precision_results), np.std(precision_results)\n",
    "recall_mean, recall_std = np.mean(recall_results), np.std(recall_results)\n",
    "f1_mean, f1_std = np.mean(f1_results), np.std(f1_results)\n",
    "auc_mean, auc_std = np.mean(auc_result), np.std(auc_result)\n",
    "\n",
    "# now get min/max directly from each list\n",
    "max_accuracy             = max(accuracy_results)\n",
    "min_accuracy             = min(accuracy_results)\n",
    "max_balanced_accuracy    = max(accuracy_results_balanced)\n",
    "min_balanced_accuracy    = min(accuracy_results_balanced)\n",
    "max_precision            = max(precision_results)\n",
    "min_precision            = min(precision_results)\n",
    "max_recall               = max(recall_results)\n",
    "min_recall               = min(recall_results)\n",
    "max_f1                   = max(f1_results)\n",
    "min_f1                   = min(f1_results)\n",
    "max_auc                  = max(auc_result)\n",
    "min_auc                  = min(auc_result)\n",
    "\n",
    "# È°ØÁ§∫ÁµêÊûú\n",
    "print('---------------------------------------------------------------')\n",
    "print(accuracy_results)\n",
    "print(accuracy_results_balanced)\n",
    "print(precision_results)\n",
    "print(recall_results)\n",
    "print(f1_results)\n",
    "print(auc_result)\n",
    "print('---------------------------------------------------------------')\n",
    "print(f\"Accuracy: Mean = {accuracy_mean:.4f}, Std = {accuracy_std:.4f}\")\n",
    "print(f\"Balanced Accuracy: Mean = {accuracy_mean_balanced:.4f}, Std = {accuracy_std_balanced:.4f}\")\n",
    "print(f\"Precision: Mean = {precision_mean:.4f}, Std = {precision_std:.4f}\")\n",
    "print(f\"Recall: Mean = {recall_mean:.4f}, Std = {recall_std:.4f}\")\n",
    "print(f\"F1: Mean = {f1_mean:.4f}, Std = {f1_std:.4f}\")\n",
    "print(f\"AUC: Mean = {auc_mean:.4f}, Std = {auc_std:.4f}\")\n",
    "print('---------------------------------------------------------------')\n",
    "print(f\"Max Accuracy:             {max_accuracy:.4f}\")\n",
    "print(f\"Max Balanced Accuracy:    {max_balanced_accuracy:.4f}\")\n",
    "print(f\"Max Precision:            {max_precision:.4f}\")\n",
    "print(f\"Max Recall:               {max_recall:.4f}\")\n",
    "print(f\"Max F1:                   {max_f1:.4f}\")\n",
    "print(f\"Max AUC:                  {max_auc:.4f}\")\n",
    "print('---------------------------------------------------------------')\n",
    "print(f\"Min Accuracy:             {min_accuracy:.4f}\")\n",
    "print(f\"Min Balanced Accuracy:    {min_balanced_accuracy:.4f}\")\n",
    "print(f\"Min Precision:            {min_precision:.4f}\")\n",
    "print(f\"Min Recall:               {min_recall:.4f}\")\n",
    "print(f\"Min F1:                   {min_f1:.4f}\")\n",
    "print(f\"Min AUC:                  {min_auc:.4f}\")\n",
    "print('---------------------------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
